{
  "metadata": {
    "created_at": "2025-07-01T11:42:33.096265",
    "version": "1.0.0",
    "description": "RAG System Evaluation Dataset",
    "total_qa_pairs": 12,
    "total_retrieval_tests": 8,
    "total_generation_tests": 6,
    "total_performance_tests": 4,
    "categories": [
      "DevOps",
      "AI/ML",
      "Infrastructure"
    ],
    "difficulty_levels": [
      "intermediate",
      "beginner",
      "advanced"
    ]
  },
  "documents": [
    {
      "id": "doc_001",
      "title": "Introduction to Large Language Models",
      "content": "Large Language Models (LLMs) are neural networks trained on vast amounts of text data. They can understand and generate human-like text across various domains. Modern LLMs like GPT, BERT, and T5 have revolutionized natural language processing.",
      "metadata": {
        "category": "AI/ML",
        "difficulty": "beginner"
      }
    },
    {
      "id": "doc_002",
      "title": "RAG System Architecture",
      "content": "Retrieval-Augmented Generation (RAG) combines information retrieval with text generation. The system first retrieves relevant documents from a knowledge base, then uses this context to generate accurate, grounded responses. Key components include document embedding, vector storage, and LLM integration.",
      "metadata": {
        "category": "AI/ML",
        "difficulty": "intermediate"
      }
    },
    {
      "id": "doc_003",
      "title": "Vector Databases and Embeddings",
      "content": "Vector databases store high-dimensional embeddings that represent semantic meaning of text. Popular options include Pinecone, Weaviate, and pgvector. Embeddings are created using models like OpenAI's text-embedding-ada-002 or open-source alternatives like FastEmbed.",
      "metadata": {
        "category": "Infrastructure",
        "difficulty": "intermediate"
      }
    },
    {
      "id": "doc_004",
      "title": "LLM Fine-tuning Techniques",
      "content": "Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA and QLoRA allow adaptation of large models with minimal computational resources. The Unsloth framework provides 2x faster training and 70% memory reduction. Techniques include DPO and ORPO for preference optimization.",
      "metadata": {
        "category": "AI/ML",
        "difficulty": "advanced"
      }
    },
    {
      "id": "doc_005",
      "title": "Docker and Containerization",
      "content": "Docker containers provide consistent deployment environments across development and production. Docker Compose orchestrates multi-service applications. Best practices include multi-stage builds, image optimization, and proper secret management.",
      "metadata": {
        "category": "DevOps",
        "difficulty": "intermediate"
      }
    },
    {
      "id": "doc_006",
      "title": "LangSmith Observability",
      "content": "LangSmith provides comprehensive observability for LLM applications. Features include request tracing, performance monitoring, debugging workflows, and analytics. Integration with LangChain enables automatic instrumentation and detailed insights into AI system behavior.",
      "metadata": {
        "category": "Monitoring",
        "difficulty": "intermediate"
      }
    },
    {
      "id": "doc_007",
      "title": "Next.js and React Development",
      "content": "Next.js 13+ introduces the App Router with server components, enabling better performance and developer experience. Features include automatic code splitting, image optimization, and built-in CSS support. Tailwind CSS provides utility-first styling for rapid UI development.",
      "metadata": {
        "category": "Frontend",
        "difficulty": "intermediate"
      }
    },
    {
      "id": "doc_008",
      "title": "FastAPI Backend Development",
      "content": "FastAPI is a modern Python web framework for building APIs with automatic documentation. It provides async/await support, type hints validation, and OpenAPI schema generation. Integration with Pydantic ensures data validation and serialization.",
      "metadata": {
        "category": "Backend",
        "difficulty": "intermediate"
      }
    }
  ],
  "qa_pairs": [
    {
      "question": "What are Large Language Models?",
      "reference_answer": "Large Language Models (LLMs) are neural networks trained on vast amounts of text data. They can understand and generate human-like text across various domains.",
      "relevant_doc_ids": [
        "doc_001"
      ],
      "difficulty": "beginner",
      "category": "AI/ML",
      "evaluation_aspects": [
        "factual_accuracy",
        "completeness"
      ],
      "id": "qa_001",
      "created_at": "2025-07-01T11:42:33.096265"
    },
    {
      "question": "Which models are examples of modern LLMs?",
      "reference_answer": "Modern LLMs include GPT, BERT, and T5, which have revolutionized natural language processing.",
      "relevant_doc_ids": [
        "doc_001"
      ],
      "difficulty": "beginner",
      "category": "AI/ML",
      "evaluation_aspects": [
        "factual_accuracy",
        "specific_knowledge"
      ],
      "id": "qa_002",
      "created_at": "2025-07-01T11:42:33.096265"
    },
    {
      "question": "What is Retrieval-Augmented Generation?",
      "reference_answer": "Retrieval-Augmented Generation (RAG) combines information retrieval with text generation. The system first retrieves relevant documents from a knowledge base, then uses this context to generate accurate, grounded responses.",
      "relevant_doc_ids": [
        "doc_002"
      ],
      "difficulty": "intermediate",
      "category": "AI/ML",
      "evaluation_aspects": [
        "conceptual_understanding",
        "technical_accuracy"
      ],
      "id": "qa_003",
      "created_at": "2025-07-01T11:42:33.096265"
    },
    {
      "question": "What are the key components of a RAG system?",
      "reference_answer": "Key components of a RAG system include document embedding, vector storage, and LLM integration.",
      "relevant_doc_ids": [
        "doc_002"
      ],
      "difficulty": "intermediate",
      "category": "AI/ML",
      "evaluation_aspects": [
        "completeness",
        "technical_detail"
      ],
      "id": "qa_004",
      "created_at": "2025-07-01T11:42:33.096265"
    },
    {
      "question": "What are vector databases used for?",
      "reference_answer": "Vector databases store high-dimensional embeddings that represent semantic meaning of text.",
      "relevant_doc_ids": [
        "doc_003"
      ],
      "difficulty": "intermediate",
      "category": "Infrastructure",
      "evaluation_aspects": [
        "technical_accuracy",
        "clarity"
      ],
      "id": "qa_005",
      "created_at": "2025-07-01T11:42:33.096265"
    },
    {
      "question": "What are some popular vector database options?",
      "reference_answer": "Popular vector database options include Pinecone, Weaviate, and pgvector.",
      "relevant_doc_ids": [
        "doc_003"
      ],
      "difficulty": "intermediate",
      "category": "Infrastructure",
      "evaluation_aspects": [
        "factual_accuracy",
        "specific_knowledge"
      ],
      "id": "qa_006",
      "created_at": "2025-07-01T11:42:33.096265"
    },
    {
      "question": "What is Parameter-Efficient Fine-Tuning?",
      "reference_answer": "Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA and QLoRA allow adaptation of large models with minimal computational resources.",
      "relevant_doc_ids": [
        "doc_004"
      ],
      "difficulty": "advanced",
      "category": "AI/ML",
      "evaluation_aspects": [
        "technical_accuracy",
        "conceptual_understanding"
      ],
      "id": "qa_007",
      "created_at": "2025-07-01T11:42:33.096265"
    },
    {
      "question": "What benefits does the Unsloth framework provide?",
      "reference_answer": "The Unsloth framework provides 2x faster training and 70% memory reduction for LLM fine-tuning.",
      "relevant_doc_ids": [
        "doc_004"
      ],
      "difficulty": "advanced",
      "category": "AI/ML",
      "evaluation_aspects": [
        "specific_knowledge",
        "quantitative_accuracy"
      ],
      "id": "qa_008",
      "created_at": "2025-07-01T11:42:33.096265"
    },
    {
      "question": "What are the benefits of using Docker containers?",
      "reference_answer": "Docker containers provide consistent deployment environments across development and production.",
      "relevant_doc_ids": [
        "doc_005"
      ],
      "difficulty": "intermediate",
      "category": "DevOps",
      "evaluation_aspects": [
        "practical_understanding",
        "clarity"
      ],
      "id": "qa_009",
      "created_at": "2025-07-01T11:42:33.096265"
    },
    {
      "question": "What is Docker Compose used for?",
      "reference_answer": "Docker Compose orchestrates multi-service applications.",
      "relevant_doc_ids": [
        "doc_005"
      ],
      "difficulty": "intermediate",
      "category": "DevOps",
      "evaluation_aspects": [
        "technical_accuracy",
        "conciseness"
      ],
      "id": "qa_010",
      "created_at": "2025-07-01T11:42:33.096265"
    },
    {
      "question": "How do vector databases and embeddings work together in RAG systems?",
      "reference_answer": "Vector databases store high-dimensional embeddings that represent semantic meaning of text. In RAG systems, these embeddings enable efficient retrieval of relevant documents from the knowledge base, which are then used as context for text generation.",
      "relevant_doc_ids": [
        "doc_002",
        "doc_003"
      ],
      "difficulty": "advanced",
      "category": "AI/ML",
      "evaluation_aspects": [
        "synthesis",
        "cross_document_reasoning",
        "technical_accuracy"
      ],
      "id": "qa_011",
      "created_at": "2025-07-01T11:42:33.096265"
    },
    {
      "question": "What technologies are needed for a complete RAG implementation?",
      "reference_answer": "A complete RAG implementation requires vector databases for storing embeddings, LLM frameworks for generation, containerization with Docker for deployment, and observability tools like LangSmith for monitoring. Frontend frameworks like Next.js and backend APIs with FastAPI complete the full-stack solution.",
      "relevant_doc_ids": [
        "doc_002",
        "doc_003",
        "doc_005",
        "doc_006",
        "doc_007",
        "doc_008"
      ],
      "difficulty": "advanced",
      "category": "AI/ML",
      "evaluation_aspects": [
        "comprehensive_understanding",
        "integration_knowledge",
        "system_thinking"
      ],
      "id": "qa_012",
      "created_at": "2025-07-01T11:42:33.096265"
    }
  ],
  "retrieval_tests": [
    {
      "query": "large language models neural networks",
      "expected_doc_ids": [
        "doc_001"
      ],
      "test_type": "exact_match",
      "description": "Direct keyword match should retrieve LLM document",
      "test_id": "retrieval_test_001",
      "created_at": "2025-07-01T11:42:33.096265"
    },
    {
      "query": "RAG retrieval augmented generation",
      "expected_doc_ids": [
        "doc_002"
      ],
      "test_type": "exact_match",
      "description": "RAG terminology should retrieve RAG architecture document",
      "test_id": "retrieval_test_002",
      "created_at": "2025-07-01T11:42:33.096265"
    },
    {
      "query": "LoRA QLoRA fine-tuning",
      "expected_doc_ids": [
        "doc_004"
      ],
      "test_type": "exact_match",
      "description": "Fine-tuning terms should retrieve fine-tuning document",
      "test_id": "retrieval_test_003",
      "created_at": "2025-07-01T11:42:33.096265"
    },
    {
      "query": "semantic meaning embeddings",
      "expected_doc_ids": [
        "doc_003"
      ],
      "test_type": "semantic_match",
      "description": "Semantic concepts should retrieve vector database document",
      "test_id": "retrieval_test_004",
      "created_at": "2025-07-01T11:42:33.096265"
    },
    {
      "query": "machine learning algorithms",
      "unexpected_doc_ids": [
        "doc_005",
        "doc_007",
        "doc_008"
      ],
      "test_type": "negative_match",
      "description": "General ML query should not retrieve DevOps/Frontend docs",
      "test_id": "retrieval_test_005",
      "created_at": "2025-07-01T11:42:33.096265"
    },
    {
      "query": "web development frameworks",
      "unexpected_doc_ids": [
        "doc_001",
        "doc_004"
      ],
      "test_type": "negative_match",
      "description": "Web dev query should not retrieve AI/ML specific docs",
      "test_id": "retrieval_test_006",
      "created_at": "2025-07-01T11:42:33.096265"
    },
    {
      "query": "AI system deployment and monitoring",
      "expected_doc_ids": [
        "doc_005",
        "doc_006"
      ],
      "min_retrieved": 2,
      "test_type": "multi_document",
      "description": "Should retrieve both Docker and LangSmith documents",
      "test_id": "retrieval_test_007",
      "created_at": "2025-07-01T11:42:33.096265"
    },
    {
      "query": "full-stack AI application development",
      "expected_doc_ids": [
        "doc_002",
        "doc_007",
        "doc_008"
      ],
      "min_retrieved": 2,
      "test_type": "multi_document",
      "description": "Should retrieve documents about RAG, frontend, and backend",
      "test_id": "retrieval_test_008",
      "created_at": "2025-07-01T11:42:33.096265"
    }
  ],
  "generation_tests": [
    {
      "question": "What performance improvements does Unsloth provide?",
      "context": "The Unsloth framework provides 2x faster training and 70% memory reduction. Techniques include DPO and ORPO for preference optimization.",
      "expected_elements": [
        "2x faster",
        "70% memory reduction"
      ],
      "test_type": "faithfulness",
      "description": "Answer should include specific quantitative claims from context",
      "test_id": "generation_test_001",
      "created_at": "2025-07-01T11:42:33.096265"
    },
    {
      "question": "What are examples of modern LLMs?",
      "context": "Modern LLMs like GPT, BERT, and T5 have revolutionized natural language processing.",
      "expected_elements": [
        "GPT",
        "BERT",
        "T5"
      ],
      "test_type": "faithfulness",
      "description": "Answer should mention specific models from context",
      "test_id": "generation_test_002",
      "created_at": "2025-07-01T11:42:33.096265"
    },
    {
      "question": "How does RAG work?",
      "irrelevant_elements": [
        "Docker",
        "Next.js",
        "FastAPI"
      ],
      "test_type": "relevance",
      "description": "RAG explanation should not include unrelated technologies",
      "test_id": "generation_test_003",
      "created_at": "2025-07-01T11:42:33.096265"
    },
    {
      "question": "What is Docker used for?",
      "relevant_elements": [
        "containers",
        "deployment",
        "environments"
      ],
      "test_type": "relevance",
      "description": "Docker answer should focus on containerization concepts",
      "test_id": "generation_test_004",
      "created_at": "2025-07-01T11:42:33.096265"
    },
    {
      "question": "What are the key components of a RAG system?",
      "required_elements": [
        "retrieval",
        "generation",
        "embeddings",
        "vector storage"
      ],
      "test_type": "completeness",
      "description": "Should cover main RAG components",
      "test_id": "generation_test_005",
      "created_at": "2025-07-01T11:42:33.096265"
    },
    {
      "question": "What are vector databases used for?",
      "required_elements": [
        "embeddings",
        "semantic meaning",
        "high-dimensional"
      ],
      "test_type": "completeness",
      "description": "Should explain vector database purpose and characteristics",
      "test_id": "generation_test_006",
      "created_at": "2025-07-01T11:42:33.096265"
    }
  ],
  "performance_tests": [
    {
      "test_id": "perf_001",
      "test_type": "latency",
      "description": "Single query response time",
      "query": "What is a large language model?",
      "max_response_time_ms": 5000,
      "target_response_time_ms": 2000
    },
    {
      "test_id": "perf_002",
      "test_type": "throughput",
      "description": "Concurrent query handling",
      "concurrent_queries": 10,
      "queries": [
        "What is RAG?",
        "What is RAG?",
        "What is RAG?",
        "What is RAG?",
        "What is RAG?",
        "What is RAG?",
        "What is RAG?",
        "What is RAG?",
        "What is RAG?",
        "What is RAG?"
      ],
      "max_total_time_ms": 15000
    },
    {
      "test_id": "perf_003",
      "test_type": "memory_usage",
      "description": "Memory consumption during processing",
      "query": "Explain vector databases and embeddings in detail",
      "max_memory_mb": 1024,
      "baseline_memory_mb": 256
    },
    {
      "test_id": "perf_004",
      "test_type": "scalability",
      "description": "Performance under load",
      "load_levels": [
        1,
        5,
        10,
        20
      ],
      "sample_query": "How does fine-tuning work?",
      "max_degradation_percent": 50
    }
  ]
}